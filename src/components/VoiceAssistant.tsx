import React, { useState, useRef, useEffect } from 'react';
import { Mic, MicOff, Volume2, VolumeX, Languages, Settings } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { useToast } from '@/hooks/use-toast';
import '../types/speech.d.ts';

interface VoiceAssistantProps {
  className?: string;
}

type Language = 'en' | 'te';
type ConversationState = 'idle' | 'listening' | 'processing' | 'speaking';

interface Message {
  id: string;
  text: string;
  language: Language;
  timestamp: Date;
  type: 'user' | 'assistant';
}

const VoiceAssistant: React.FC<VoiceAssistantProps> = ({ className = '' }) => {
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentLanguage, setCurrentLanguage] = useState<Language>('en');
  const [conversationState, setConversationState] = useState<ConversationState>('idle');
  const [messages, setMessages] = useState<Message[]>([]);
  const [isConfigured, setIsConfigured] = useState(false);
  
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const synthRef = useRef<SpeechSynthesis | null>(null);
  const { toast } = useToast();

  const languageConfig = {
    en: {
      name: 'English',
      flag: 'üá∫üá∏',
      speechLang: 'en-US',
      greetings: [
        'Hello! I\'m your AgroSight assistant. How can I help you with farming today?',
        'Hi there! Ask me about crop yields, market prices, or farming loans.',
        'Welcome to AgroSight! I\'m here to assist with your agricultural questions.'
      ]
    },
    te: {
      name: 'Telugu',
      flag: 'üáÆüá≥',
      speechLang: 'te-IN',
      greetings: [
        '‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç! ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä ‡∞Ö‡∞ó‡±ç‡∞∞‡±ã‡∞∏‡±à‡∞ü‡±ç ‡∞Ö‡∞∏‡∞ø‡∞∏‡±ç‡∞ü‡±Ü‡∞Ç‡∞ü‡±ç. ‡∞à ‡∞∞‡±ã‡∞ú‡±Å ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡∞æ‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞é‡∞≤‡∞æ ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞®‡±Å?',
        '‡∞π‡∞≤‡±ã! ‡∞™‡∞Ç‡∞ü ‡∞¶‡∞ø‡∞ó‡±Å‡∞¨‡∞°‡±Å‡∞≤‡±Å, ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±Ü‡∞ü‡±ç ‡∞ß‡∞∞‡∞≤‡±Å ‡∞≤‡±á‡∞¶‡∞æ ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡∞æ‡∞Ø ‡∞∞‡±Å‡∞£‡∞æ‡∞≤ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø.',
        '‡∞Ö‡∞ó‡±ç‡∞∞‡±ã‡∞∏‡±à‡∞ü‡±ç‚Äå‡∞ï‡±Å ‡∞∏‡±ç‡∞µ‡∞æ‡∞ó‡∞§‡∞Ç! ‡∞Æ‡±Ä ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡∞æ‡∞Ø ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡∞§‡±ã ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞®‡±á‡∞®‡±Å ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.'
      ]
    }
  };

  // Initialize speech recognition
  useEffect(() => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      
      if (recognitionRef.current) {
        recognitionRef.current.continuous = false;
        recognitionRef.current.interimResults = false;
        recognitionRef.current.lang = languageConfig[currentLanguage].speechLang;
        
        recognitionRef.current.onstart = () => {
          setIsListening(true);
          setConversationState('listening');
        };
        
        recognitionRef.current.onend = () => {
          setIsListening(false);
          if (conversationState === 'listening') {
            setConversationState('idle');
          }
        };
        
        recognitionRef.current.onresult = (event) => {
          const transcript = event.results[0][0].transcript;
          handleUserInput(transcript);
        };
        
        recognitionRef.current.onerror = (event) => {
          console.error('Speech recognition error:', event.error);
          setIsListening(false);
          setConversationState('idle');
          toast({
            title: "Speech Recognition Error",
            description: "Could not process your speech. Please try again.",
            variant: "destructive"
          });
        };
      }
    } else {
      toast({
        title: "Speech Recognition Not Supported",
        description: "Your browser doesn't support speech recognition.",
        variant: "destructive"
      });
    }

    // Initialize speech synthesis
    synthRef.current = window.speechSynthesis;
    
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
      if (synthRef.current) {
        synthRef.current.cancel();
      }
    };
  }, [currentLanguage]);

  // Update language when changed
  useEffect(() => {
    if (recognitionRef.current) {
      recognitionRef.current.lang = languageConfig[currentLanguage].speechLang;
    }
  }, [currentLanguage]);

  const startListening = async () => {
    try {
      // Request microphone permission
      await navigator.mediaDevices.getUserMedia({ audio: true });
      
      if (recognitionRef.current && !isListening) {
        recognitionRef.current.start();
      }
    } catch (error) {
      console.error('Microphone access denied:', error);
      toast({
        title: "Microphone Access Required",
        description: "Please allow microphone access to use voice features.",
        variant: "destructive"
      });
    }
  };

  const stopListening = () => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
    }
  };

  const handleUserInput = async (transcript: string) => {
    const userMessage: Message = {
      id: Date.now().toString(),
      text: transcript,
      language: currentLanguage,
      timestamp: new Date(),
      type: 'user'
    };
    
    setMessages(prev => [...prev, userMessage]);
    setConversationState('processing');
    
    // Process the input and generate response
    const response = await generateResponse(transcript, currentLanguage);
    
    const assistantMessage: Message = {
      id: (Date.now() + 1).toString(),
      text: response,
      language: currentLanguage,
      timestamp: new Date(),
      type: 'assistant'
    };
    
    setMessages(prev => [...prev, assistantMessage]);
    
    // Speak the response
    await speakText(response, currentLanguage);
    setConversationState('idle');
  };

  const generateResponse = async (input: string, language: Language): Promise<string> => {
    // Simple rule-based responses for demo
    // In production, this would connect to OpenAI or similar AI service
    const lowerInput = input.toLowerCase();
    
    if (language === 'en') {
      if (lowerInput.includes('yield') || lowerInput.includes('crop')) {
        return "I can help you predict crop yields. Would you like to use our Yield Predictor tool? I can guide you through entering your crop details, soil conditions, and weather data.";
      } else if (lowerInput.includes('price') || lowerInput.includes('market')) {
        return "I can provide market price forecasts for your crops. Our Market Forecast tool analyzes current trends and predicts future prices. What crop are you interested in?";
      } else if (lowerInput.includes('loan') || lowerInput.includes('finance')) {
        return "I can help you calculate agricultural loans. Our Loan Calculator can determine your eligibility and monthly payments. What type of loan are you looking for?";
      } else if (lowerInput.includes('weather')) {
        return "Weather is crucial for farming. I recommend checking our weather integration features that provide detailed forecasts for your location.";
      } else {
        return "I'm here to help with your agricultural needs. You can ask me about crop yields, market prices, loans, or weather forecasts. How can I assist you today?";
      }
    } else {
      // Telugu responses
      if (lowerInput.includes('‡∞¶‡∞ø‡∞ó‡±Å‡∞¨‡∞°‡∞ø') || lowerInput.includes('‡∞™‡∞Ç‡∞ü')) {
        return "‡∞®‡±á‡∞®‡±Å ‡∞™‡∞Ç‡∞ü ‡∞¶‡∞ø‡∞ó‡±Å‡∞¨‡∞°‡±Å‡∞≤‡∞®‡±Å ‡∞Ö‡∞Ç‡∞ö‡∞®‡∞æ ‡∞µ‡±á‡∞Ø‡∞°‡∞Ç‡∞≤‡±ã ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞®‡±Å. ‡∞Æ‡∞æ ‡∞Ø‡±Ä‡∞≤‡±ç‡∞°‡±ç ‡∞™‡±ç‡∞∞‡∞ø‡∞°‡∞ø‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞ü‡±Ç‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ? ‡∞Æ‡±Ä ‡∞™‡∞Ç‡∞ü ‡∞µ‡∞ø‡∞µ‡∞∞‡∞æ‡∞≤‡±Å, ‡∞Æ‡∞ü‡±ç‡∞ü‡∞ø ‡∞™‡∞∞‡∞ø‡∞∏‡±ç‡∞•‡∞ø‡∞§‡±Å‡∞≤‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞µ‡∞æ‡∞§‡∞æ‡∞µ‡∞∞‡∞£ ‡∞°‡±á‡∞ü‡∞æ‡∞®‡±Å ‡∞®‡∞Æ‡±ã‡∞¶‡±Å ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç‡∞≤‡±ã ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡∞ø‡∞Æ‡±ç‡∞Æ‡∞≤‡±ç‡∞®‡∞ø ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ó‡∞®‡∞ø‡∞∞‡±ç‡∞¶‡±á‡∞∂‡∞Ç ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞®‡±Å.";
      } else if (lowerInput.includes('‡∞ß‡∞∞') || lowerInput.includes('‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±Ü‡∞ü‡±ç')) {
        return "‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä ‡∞™‡∞Ç‡∞ü‡∞≤‡∞ï‡±Å ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±Ü‡∞ü‡±ç ‡∞ß‡∞∞ ‡∞Ö‡∞Ç‡∞ö‡∞®‡∞æ‡∞≤‡∞®‡±Å ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞ó‡∞≤‡∞®‡±Å. ‡∞Æ‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±Ü‡∞ü‡±ç ‡∞´‡±ã‡∞∞‡±ç‚Äå‡∞ï‡∞æ‡∞∏‡±ç‡∞ü‡±ç ‡∞ü‡±Ç‡∞≤‡±ç ‡∞™‡±ç‡∞∞‡∞∏‡±ç‡∞§‡±Å‡∞§ ‡∞ü‡±ç‡∞∞‡±Ü‡∞Ç‡∞°‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞µ‡∞ø‡∞∂‡±ç‡∞≤‡±á‡∞∑‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞≠‡∞µ‡∞ø‡∞∑‡±ç‡∞Ø‡∞§‡±ç ‡∞ß‡∞∞‡∞≤‡∞®‡±Å ‡∞Ö‡∞Ç‡∞ö‡∞®‡∞æ ‡∞µ‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞è ‡∞™‡∞Ç‡∞ü ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç?";
      } else if (lowerInput.includes('‡∞∞‡±Å‡∞£‡∞Ç') || lowerInput.includes('‡∞´‡±à‡∞®‡∞æ‡∞®‡±ç‡∞∏‡±ç')) {
        return "‡∞®‡±á‡∞®‡±Å ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡∞æ‡∞Ø ‡∞∞‡±Å‡∞£‡∞æ‡∞≤‡∞®‡±Å ‡∞≤‡±Ü‡∞ï‡±ç‡∞ï‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç‡∞≤‡±ã ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞®‡±Å. ‡∞Æ‡∞æ ‡∞≤‡±ã‡∞®‡±ç ‡∞ï‡∞æ‡∞≤‡∞ø‡∞ï‡±ç‡∞Ø‡±Å‡∞≤‡±á‡∞ü‡∞∞‡±ç ‡∞Æ‡±Ä ‡∞Ö‡∞∞‡±ç‡∞π‡∞§ ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞®‡±Ü‡∞≤‡∞µ‡∞æ‡∞∞‡±Ä ‡∞ö‡±Ü‡∞≤‡±ç‡∞≤‡∞ø‡∞Ç‡∞™‡±Å‡∞≤‡∞®‡±Å ‡∞®‡∞ø‡∞∞‡±ç‡∞£‡∞Ø‡∞ø‡∞Ç‡∞ö‡∞ó‡∞≤‡∞¶‡±Å. ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞é‡∞≤‡∞æ‡∞Ç‡∞ü‡∞ø ‡∞∞‡±Å‡∞£‡∞Ç ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡∞ø?";
      } else if (lowerInput.includes('‡∞µ‡∞æ‡∞§‡∞æ‡∞µ‡∞∞‡∞£‡∞Ç')) {
        return "‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡∞æ‡∞Ø‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞µ‡∞æ‡∞§‡∞æ‡∞µ‡∞∞‡∞£‡∞Ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø‡∞Æ‡±à‡∞®‡∞¶‡∞ø. ‡∞Æ‡±Ä ‡∞™‡±ç‡∞∞‡∞æ‡∞Ç‡∞§‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞µ‡∞ø‡∞µ‡∞∞‡∞£‡∞æ‡∞§‡±ç‡∞Æ‡∞ï ‡∞Ö‡∞Ç‡∞ö‡∞®‡∞æ‡∞≤‡∞®‡±Å ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡±á ‡∞Æ‡∞æ ‡∞µ‡∞æ‡∞§‡∞æ‡∞µ‡∞∞‡∞£ ‡∞á‡∞Ç‡∞ü‡∞ø‡∞ó‡±ç‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞´‡±Ä‡∞ö‡∞∞‡±ç‡∞≤‡∞®‡±Å ‡∞§‡∞®‡∞ø‡∞ñ‡±Ä ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡∞ø ‡∞®‡±á‡∞®‡±Å ‡∞∏‡∞ø‡∞´‡∞æ‡∞∞‡±ç‡∞∏‡±Å ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.";
      } else {
        return "‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡∞æ‡∞Ø ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞æ‡∞≤‡∞§‡±ã ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å. ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞™‡∞Ç‡∞ü ‡∞¶‡∞ø‡∞ó‡±Å‡∞¨‡∞°‡±Å‡∞≤‡±Å, ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±Ü‡∞ü‡±ç ‡∞ß‡∞∞‡∞≤‡±Å, ‡∞∞‡±Å‡∞£‡∞æ‡∞≤‡±Å ‡∞≤‡±á‡∞¶‡∞æ ‡∞µ‡∞æ‡∞§‡∞æ‡∞µ‡∞∞‡∞£ ‡∞Ö‡∞Ç‡∞ö‡∞®‡∞æ‡∞≤ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞µ‡∞ö‡±ç‡∞ö‡±Å. ‡∞à ‡∞∞‡±ã‡∞ú‡±Å ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞é‡∞≤‡∞æ ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞®‡±Å?";
      }
    }
  };

  const speakText = async (text: string, language: Language): Promise<void> => {
    return new Promise((resolve) => {
      if (synthRef.current) {
        setIsSpeaking(true);
        setConversationState('speaking');
        
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = languageConfig[language].speechLang;
        utterance.rate = 0.9;
        utterance.pitch = 1;
        
        utterance.onend = () => {
          setIsSpeaking(false);
          setConversationState('idle');
          resolve();
        };
        
        utterance.onerror = () => {
          setIsSpeaking(false);
          setConversationState('idle');
          resolve();
        };
        
        synthRef.current.speak(utterance);
      } else {
        resolve();
      }
    });
  };

  const toggleListening = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  const stopSpeaking = () => {
    if (synthRef.current) {
      synthRef.current.cancel();
      setIsSpeaking(false);
      setConversationState('idle');
    }
  };

  const startGreeting = async () => {
    const greetings = languageConfig[currentLanguage].greetings;
    const randomGreeting = greetings[Math.floor(Math.random() * greetings.length)];
    
    const greetingMessage: Message = {
      id: Date.now().toString(),
      text: randomGreeting,
      language: currentLanguage,
      timestamp: new Date(),
      type: 'assistant'
    };
    
    setMessages(prev => [...prev, greetingMessage]);
    await speakText(randomGreeting, currentLanguage);
  };

  const getStateIcon = () => {
    switch (conversationState) {
      case 'listening':
        return <Mic className="h-5 w-5 animate-pulse text-red-500" />;
      case 'processing':
        return <Settings className="h-5 w-5 animate-spin text-blue-500" />;
      case 'speaking':
        return <Volume2 className="h-5 w-5 animate-pulse text-green-500" />;
      default:
        return <MicOff className="h-5 w-5 text-muted-foreground" />;
    }
  };

  return (
    <Card className={`w-full max-w-md ${className}`}>
      <CardHeader className="pb-4">
        <CardTitle className="flex items-center gap-2">
          <Languages className="h-5 w-5" />
          Voice Assistant
          <Badge variant="secondary" className="ml-auto">
            {languageConfig[currentLanguage].flag} {languageConfig[currentLanguage].name}
          </Badge>
        </CardTitle>
      </CardHeader>
      
      <CardContent className="space-y-4">
        {/* Language Selection */}
        <div className="flex items-center gap-2">
          <Select value={currentLanguage} onValueChange={(value: Language) => setCurrentLanguage(value)}>
            <SelectTrigger className="w-full">
              <SelectValue />
            </SelectTrigger>
            <SelectContent>
              <SelectItem value="en">üá∫üá∏ English</SelectItem>
              <SelectItem value="te">üáÆüá≥ Telugu</SelectItem>
            </SelectContent>
          </Select>
        </div>

        {/* Control Buttons */}
        <div className="flex gap-2">
          <Button
            onClick={toggleListening}
            disabled={conversationState === 'processing' || conversationState === 'speaking'}
            variant={isListening ? "destructive" : "default"}
            className="flex-1"
          >
            {getStateIcon()}
            {isListening ? "Stop" : "Listen"}
          </Button>
          
          <Button
            onClick={stopSpeaking}
            disabled={!isSpeaking}
            variant="outline"
            size="icon"
          >
            <VolumeX className="h-4 w-4" />
          </Button>
          
          <Button
            onClick={startGreeting}
            disabled={conversationState !== 'idle'}
            variant="outline"
            size="icon"
          >
            <Volume2 className="h-4 w-4" />
          </Button>
        </div>

        {/* Status Display */}
        <div className="text-center">
          <Badge variant={conversationState === 'idle' ? 'secondary' : 'default'}>
            {conversationState === 'idle' && 'Ready to help'}
            {conversationState === 'listening' && 'Listening...'}
            {conversationState === 'processing' && 'Processing...'}
            {conversationState === 'speaking' && 'Speaking...'}
          </Badge>
        </div>

        {/* Recent Messages */}
        {messages.length > 0 && (
          <div className="space-y-2 max-h-40 overflow-y-auto">
            {messages.slice(-3).map((message) => (
              <div
                key={message.id}
                className={`text-sm p-2 rounded ${
                  message.type === 'user'
                    ? 'bg-primary text-primary-foreground ml-4'
                    : 'bg-muted mr-4'
                }`}
              >
                {message.text}
              </div>
            ))}
          </div>
        )}

        {/* Setup Notice */}
        {!isConfigured && (
          <div className="text-xs text-muted-foreground text-center">
            Note: For enhanced AI responses, connect to Supabase and configure OpenAI integration.
          </div>
        )}
      </CardContent>
    </Card>
  );
};

export default VoiceAssistant;